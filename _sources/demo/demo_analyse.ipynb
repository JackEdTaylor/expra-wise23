{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing we'll go through today is analysis of the Demo Experiment data. We have data from 14 participants who completed the German version of the experiment.\n",
    "\n",
    "**Before we can start:**\n",
    "\n",
    "**a**) [Download the data](demo_data.zip) as a `.zip` file.\n",
    "\n",
    "**b**) Extract the data from the `.zip` file to a folder that makes sense for you. The data should be stored in that location in a folder called `data`.\n",
    "\n",
    "**c**) Start a new R script in RStudio.\n",
    "\n",
    "Once everyone has completed these steps, we'll begin..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Load the Relevant Libraries\n",
    "\n",
    "These are the libraries we'll use for the analysis. Each library has a comment explaining what we will use it for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=3.5, repr.plot.height=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(readr)    # for reading the data into R\n",
    "library(purrr)    # for easily importing multiple files\n",
    "library(dplyr)    # for wrangling data (e.g., adding/renaming columns)\n",
    "library(tidyr)    # for switching between long and wide formats of data\n",
    "library(ordinal)  # for fitting CLMMs\n",
    "library(lme4)     # for fitting LMMs\n",
    "\n",
    "library(ggplot2)  # for visualising data\n",
    "theme_set(theme_bw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Import the Data\n",
    "\n",
    "Let's import the data extracted from the `.zip` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-output"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first, get a list of paths to all .csv data files\n",
    "data_paths <- list.files(\"data\", pattern=\".*\\\\.csv$\", full.names=TRUE)\n",
    "\n",
    "# now, iterate over these with `read_csv()` to import them\n",
    "# (note: col_types just says that we want these two columns to be stored as text, not numbers)\n",
    "raw_data <- map_df(data_paths, read_csv, col_types=c(participant=\"c\", frameRate=\"c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the first few rows of the data, with `print()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Format the Data\n",
    "\n",
    "Here we make two quick changes:\n",
    "\n",
    "1) We rename the column containing response variable to a short name: `resp`\n",
    "\n",
    "2) We create a new column, which will be the same variable but with the order of the responses hardcoded as a factor\n",
    "\n",
    "We store the resulting data frame in a new variable with a handy short name, `d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "d <- raw_data |>\n",
    "  # 1) rename to a shorter name\n",
    "  rename(resp = resp_scale.response) |>\n",
    "  # 2) set the factor levels explicitly\n",
    "  mutate( resp_fct = factor(resp, levels=1:5, ordered=TRUE) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding the variable as a factor in this last step is useful for the CLMM approach. It ensures that R knows the order of our Likert responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Clean the Data\n",
    "\n",
    "To remove outliers, we may want to filter by response times, or exclude participants who only provided one response for the whole experiment.\n",
    "\n",
    "Here are the rules we'll use:\n",
    "\n",
    "* *Rule a)* We will exclude participants who only pressed one button for the whole experiment\n",
    "\n",
    "* *Rule b)* We will exclude trials where participants were too fast to be paying attention (faster than 500 ms).\n",
    "\n",
    "* *Rule c)* We will exclude trials where participants were too slow to be paying attention (slower than 15 seconds).\n",
    "\n",
    "Remember that for your actual experiment, you will need to preregister these criteria.\n",
    "\n",
    "<br>\n",
    "We'll start by excluding participants who gave too few unique responses. To do this we first count the number of unique responses given by each participant. Then we can take from this a list of participants who only pressed one button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get the number of unique responses from each participant\n",
    "participant_variety <- d |>\n",
    "  # select the relevant columns\n",
    "  select(participant, resp_fct) |>\n",
    "  # get unique responses\n",
    "  distinct() |>\n",
    "  # count unique responses per participant\n",
    "  count(participant) |>\n",
    "  # sort ascendingly\n",
    "  arrange(n)\n",
    "\n",
    "# get IDs of participants who only ever pressed one button\n",
    "bad_participants <- participant_variety |>\n",
    "  filter(n==1) |>\n",
    "  pull(participant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can filter our data to not include these bad participants. This code says that we should only keep participants who are not (`!`) in (`%in%`) the vector containing bad participant IDs (`bad_participants`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# a) exclude participants who only ever pressed one button\n",
    "d_clean_a <- filter(d, ! participant %in% bad_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding participants by Response Times (RTs) is much easier. We can say we want to keep trials where participants were under the maximum RT, and above the minimum RT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# b) exclude trials that were too slow\n",
    "d_clean_b <- filter(d_clean_a, resp_scale.rt < 15)\n",
    "\n",
    "# c) exclude trials that were too fast\n",
    "d_clean_c <- filter(d_clean_b, resp_scale.rt > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to calculate the number of participants/trials lost at each step. We can do this with the `nrow()` function to count the number of rows (i.e., trials) in the dataframe. To count the number of bad participants, we can use `length()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# there were 0 participants excluded on the basis of only ever pressing one button\n",
    "print(length(bad_participants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# there were 14 trials excluded on the basis of slow responses\n",
    "print( nrow(d_clean_a) - nrow(d_clean_b) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# there were 0 trials excluded on the basis of fast responses\n",
    "print( nrow(d_clean_b) - nrow(d_clean_c) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Fit the Models!\n",
    "\n",
    "Now that the data are formatted and cleaned we can fit three models to estimate our norms:\n",
    "\n",
    "1. The traditional model (LM; `lm()`), where each item's ratings are averaged.\n",
    "\n",
    "2. A linear mixed-effects model (LMM; `lmer()`), where we norm via random effects, but still treat the ratings as continuous.\n",
    "\n",
    "3. A cumulative-link mixed-effects model (CLMM; `clmm()`), where we norm via random effects, on a latent normal distribution.\n",
    "\n",
    "<br>\n",
    "First, the traditional approach is to take a raw average of all ratings for each item. A simple way of estimating this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit traditional model\n",
    "m_lm <- lm(resp ~ 0 + word, data = data_clean_c)\n",
    "\n",
    "# extract estimates (coefficients)\n",
    "est_lm <- coef(m_lm) |>\n",
    "  # put into a tidy dataframe\n",
    "  as_tibble(rownames = \"word\") |>\n",
    "  # M, standing for mean\n",
    "  rename(M = value) |>\n",
    "  # store the model type as a variable (we'll use this later)\n",
    "  mutate(model = \"LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To norm via random effects in an LMM, we can use the `lmer()` function. We include random (varying) intercepts for participants and items, even though we will only extract estimates for items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit LMM\n",
    "m_lmm <- lmer(\n",
    "    resp ~ 1 + (1 | word) + (1 | participant),\n",
    "    data = d_clean_c\n",
    "  )\n",
    "\n",
    "# extract estimates (item random effects)\n",
    "est_lmm <- ranef(m_lmm)$word |>\n",
    "  # put into a tidy dataframe\n",
    "  as_tibble(rownames = \"word\") |>\n",
    "  # M, standing for mean\n",
    "  rename(M = `(Intercept)`) |>\n",
    "  # store the model type as a variable\n",
    "  mutate(model = \"LMM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To norm via random effects in a CLMM, we can use the `clmm()` function. We include the same randonm effects as above. The key difference is that now we are using a model more appropriate for ordinal ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit CLMM (probit-link for comparability)\n",
    "m_clmm <- clmm(\n",
    "  resp_fct ~ 1 + (1 | word) + (1 | participant),\n",
    "  data = d_clean_c,\n",
    "  link = \"probit\"\n",
    ")\n",
    "\n",
    "# extract estimates (item random effects)\n",
    "est_clmm <- ranef(m_clmm)$word |>\n",
    "  # put into a tidy dataframe\n",
    "  as_tibble(rownames = \"word\") |>\n",
    "  # M, standing for mean\n",
    "  rename(M = `(Intercept)`) |>\n",
    "  # store the model type as a variable\n",
    "  mutate(model = \"CLMM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Compare the AICs\n",
    "\n",
    "One nice comparison we can do is on the Akaike Information Criteria (AIC). This is something we touched on in the session on [Empirical Research](https://jackedtaylor.github.io/expra-wise23/introduction/empirical_research.html). The AIC, and estimators like it, can help us decide which model is most appropriate for the trade-off between model complexity and model likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "AIC(m_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "AIC(m_lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "AIC(m_clmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Join the Estimates Together\n",
    "\n",
    "Now that we have the estimates, we can join these rows together into one big dataframe, `norms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Stick the dataframes together, end-to-end\n",
    "norms <- bind_rows(est_lm, est_lmm, est_clmm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
